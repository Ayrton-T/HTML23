{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  plot\n",
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "#   basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "#   acceleratioin\n",
    "from numba import jit, njit\n",
    "\n",
    "#   learning packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor as knn\n",
    "from sklearn.preprocessing import normalize\n",
    "from lightgbm import LGBMModel, LGBMClassifier, LGBMRegressor, plot_importance\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, AdaBoostRegressor\n",
    "# from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed data\n",
    "with open('preprocessed_data/efedericisentence_bert_base/train_feature.json') as f:\n",
    "    training_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_id = [int(data['id']) for data in training_data]\n",
    "X = [data['feature'] for data in training_data]\n",
    "X = np.asarray(X, dtype=np.float32)\n",
    "Y = np.asarray([int(data['label']) for data in training_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed_data/efedericisentence_bert_base/test_feature.json') as f:\n",
    "    testing_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_id = [int(data['id']) for data in testing_data]\n",
    "X_test = [data['feature'] for data in testing_data]\n",
    "X_test = np.asarray(X_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y, test_size=0.33, random_state=8787)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4231, 5393) (2084, 5393)\n"
     ]
    }
   ],
   "source": [
    "X_test_train, X_test_eval = train_test_split(X_test, test_size=0.33, random_state=8787)\n",
    "print(X_test_train.shape, X_test_eval.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17170, 5393)\n",
      "(6315, 5393)\n",
      "(11503, 5393) (5667, 5393)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_test.shape)\n",
    "print(X_train.shape, X_eval.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.590519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1088804\n",
      "[LightGBM] [Info] Number of data points in the train set: 11503, number of used features: 5393\n",
      "[LightGBM] [Info] Start training from score 4.578719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.4, bagging_freq=10, boosting_type='rf',\n",
       "              feature_fraction=0.3, metric='mae', n_estimators=25000, n_jobs=10,\n",
       "              random_state=1234, verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_rf_model = LGBMRegressor(n_estimators=25000,\n",
    "                            verbose=1,\n",
    "                            n_jobs=10,\n",
    "                            feature_fraction=0.3,\n",
    "                            bagging_fraction=0.4,\n",
    "                            bagging_freq=10,\n",
    "                            random_state=1234,\n",
    "                            boosting_type='rf',\n",
    "                            metric='mae',)\n",
    "lgbm_rf_model.fit(X_train,Y_train)\n",
    "\n",
    "# with open(f'model/lgbm_rf_model_1.975083351596476.pickle', 'rb') as f:\n",
    "#     lgbm_rf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.91697832 4.85502455 3.21674401 5.19528259 5.36363094 5.09930997\n",
      " 4.87604682 2.98759929 3.48252934 4.79766436]\n",
      "[6 8 5 6 5 7 2 0 6 5]\n",
      "1.9750910403054105\n"
     ]
    }
   ],
   "source": [
    "# Validation random forest\n",
    "rf_pred = lgbm_rf_model.predict(X_eval)\n",
    "print(rf_pred[:10])\n",
    "print(Y_eval[:10])\n",
    "mae = np.mean(np.abs(rf_pred - Y_eval))\n",
    "print(np.mean(np.abs(rf_pred - Y_eval)))\n",
    "\n",
    "with open(f'model/lgbm_rf_model_{mae}.pickle', 'wb') as f:\n",
    "    pickle.dump(lgbm_rf_model, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_model = AdaBoostClassifier()\n",
    "ada_model = ada_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 6 2 8 6 1 0 3 9]\n",
      "[6 8 5 6 5 7 2 0 6 5]\n",
      "1.892182812775719\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "ada_pred = ada_model.predict(X_eval)\n",
    "print(ada_pred[:10])\n",
    "print(Y_eval[:10])\n",
    "mae = np.mean(np.abs(ada_pred - Y_eval))\n",
    "print(np.mean(np.abs(ada_pred - Y_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'model/ensemble_adaboost_model_{mae}.pickle', 'wb') as f:\n",
    "    pickle.dump(ada_model, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM dart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.581459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1088804\n",
      "[LightGBM] [Info] Number of data points in the train set: 11503, number of used features: 5393\n",
      "[LightGBM] [Info] Start training from score 4.578719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.4, bagging_freq=10, boosting_type='dart',\n",
       "              feature_fraction=0.3, metric='mae', n_estimators=25000, n_jobs=10,\n",
       "              random_state=1234, verbose=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_dart_model = LGBMRegressor(n_estimators=25000,\n",
    "                            verbose=1,\n",
    "                            n_jobs=10,\n",
    "                            feature_fraction=0.3,\n",
    "                            bagging_fraction=0.4,\n",
    "                            bagging_freq=10,\n",
    "                            random_state=1234,\n",
    "                            boosting_type='dart',\n",
    "                            metric='mae',)\n",
    "\n",
    "lgbm_dart_model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.93984977  4.65687972  5.29374439  4.21730308  5.45037259  5.5643454\n",
      "  3.05679004 -0.11307005  3.99976695  5.53339056]\n",
      "[6 8 5 6 5 7 2 0 6 5]\n",
      "1.5032024608614731\n"
     ]
    }
   ],
   "source": [
    "# Validation dart\n",
    "dart_pred = lgbm_dart_model.predict(X_eval)\n",
    "print(dart_pred[:10])\n",
    "print(Y_eval[:10])\n",
    "mae = np.mean(np.abs(dart_pred - Y_eval))\n",
    "print(np.mean(np.abs(dart_pred - Y_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'model/lgbm_dart_model_{mae}.pickle', 'wb') as f:\n",
    "    pickle.dump(lgbm_dart_model, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.950455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1088804\n",
      "[LightGBM] [Info] Number of data points in the train set: 11503, number of used features: 5393\n",
      "[LightGBM] [Info] Start training from score 4.578719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.4, feature_fraction=0.3, metric='mae',\n",
       "              n_estimators=2500, n_jobs=10, random_state=1234, verbose=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_gbdt_model = LGBMRegressor(n_estimators=2500,\n",
    "                            verbose=1,\n",
    "                            n_jobs=10,\n",
    "                            feature_fraction=0.3,\n",
    "                            bagging_fraction=0.4,\n",
    "                            random_state=1234,\n",
    "                            boosting_type='gbdt',\n",
    "                            metric='mae',\n",
    "                            )\n",
    "\n",
    "lgbm_gbdt_model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.73205006  4.53371631  3.97131426  4.49344836  5.18928407  5.4346011\n",
      "  3.53314585 -0.1132879   3.20598694  6.43975914]\n",
      "[6 8 5 6 5 7 2 0 6 5]\n",
      "1.5325363633330564\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "gbdt_pred = lgbm_gbdt_model.predict(X_eval)\n",
    "print(gbdt_pred[:10])\n",
    "print(Y_eval[:10])\n",
    "mae = np.mean(np.abs(gbdt_pred - Y_eval))\n",
    "print(np.mean(np.abs(gbdt_pred - Y_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'model/lgbm_gbdt_model_{mae}.pickle', 'wb') as f:\n",
    "    pickle.dump(lgbm_gbdt_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.999722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1088804\n",
      "[LightGBM] [Info] Number of data points in the train set: 11503, number of used features: 5393\n",
      "[LightGBM] [Info] Start training from score 4.578719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.4, bagging_freq=10, feature_fraction=0.3,\n",
       "              metric='mae', n_estimators=25000, n_jobs=10, random_state=1234,\n",
       "              verbose=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_gbdt_model2 = LGBMRegressor(n_estimators=25000,\n",
    "                            verbose=1,\n",
    "                            n_jobs=10,\n",
    "                            feature_fraction=0.3,\n",
    "                            bagging_fraction=0.4,\n",
    "                            bagging_freq=10,\n",
    "                            random_state=1234,\n",
    "                            boosting_type='gbdt',\n",
    "                            metric='mae',\n",
    "                            )\n",
    "\n",
    "lgbm_gbdt_model2.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.98833594  5.3094955   4.76288748  3.82490299  3.01964699  5.60997923\n",
      "  3.28107687 -0.47853178  2.87270623  5.83664299]\n",
      "[6 8 5 6 5 7 2 0 6 5]\n",
      "1.6056601414281035\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "gbdt2_pred = lgbm_gbdt_model2.predict(X_eval)\n",
    "print(gbdt2_pred[:10])\n",
    "print(Y_eval[:10])\n",
    "mae = np.mean(np.abs(gbdt2_pred - Y_eval))\n",
    "print(np.mean(np.abs(gbdt2_pred - Y_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'model/lgbm_gbdt_bag_model_{mae}.pickle', 'wb') as f:\n",
    "    pickle.dump(lgbm_gbdt_model2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.843587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1088804\n",
      "[LightGBM] [Info] Number of data points in the train set: 11503, number of used features: 5393\n",
      "[LightGBM] [Info] Start training from score 4.578719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.4, bagging_freq=10, feature_fraction=0.3,\n",
       "              metric='mae', n_estimators=2500, n_jobs=10, random_state=1234,\n",
       "              verbose=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_gbdt_model3 = LGBMRegressor(n_estimators=2500,\n",
    "                            verbose=1,\n",
    "                            n_jobs=10,\n",
    "                            feature_fraction=0.3,\n",
    "                            bagging_fraction=0.4,\n",
    "                            bagging_freq=10,\n",
    "                            random_state=1234,\n",
    "                            boosting_type='gbdt',\n",
    "                            metric='mae',\n",
    "                            )\n",
    "\n",
    "lgbm_gbdt_model3.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.98416481  5.31289515  4.76109461  3.82458854  3.01767715  5.6030891\n",
      "  3.28373797 -0.47352973  2.86887975  5.83252205]\n",
      "[6 8 5 6 5 7 2 0 6 5]\n",
      "1.6056904021808203\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "gbdt3_pred = lgbm_gbdt_model3.predict(X_eval)\n",
    "print(gbdt3_pred[:10])\n",
    "print(Y_eval[:10])\n",
    "mae = np.mean(np.abs(gbdt3_pred - Y_eval))\n",
    "print(np.mean(np.abs(gbdt3_pred - Y_eval)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# blend with small part test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2147, 5393) (4168, 5393) (2147,) (4168,)\n",
      "(11503, 5393) (13650, 5393) (13650,)\n"
     ]
    }
   ],
   "source": [
    "usecol = ['Danceability']\n",
    "df = pd.read_csv('./html2023-spring-final-project/test_predic_label.csv', usecols=usecol)\n",
    "\n",
    "Y_test = df['Danceability'].astype(int).to_numpy()\n",
    "\n",
    "X_test_train, X_test_eval, Y_test_train, Y_test_eval, id_test_train, id_test_eval = train_test_split(X_test, Y_test, testing_id,  test_size=0.66, random_state=8787)\n",
    "print(X_test_train.shape, X_test_eval.shape, Y_test_train.shape, Y_test_eval.shape)\n",
    "\n",
    "X_blend_train = np.concatenate([X_train, X_test_train])\n",
    "Y_blend_train = np.concatenate([Y_train, Y_test_train])\n",
    "print(X_blend_train.shape, Y_blend_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 7 4 6 7 6 4 0 3 9]\n",
      "[6 8 5 6 5 7 2 0 6 5]\n",
      "1.8459502382212811\n"
     ]
    }
   ],
   "source": [
    "# Adaboost\n",
    "ada_blend_model = AdaBoostClassifier()\n",
    "ada_blend_model = ada_blend_model.fit(X_blend_train, Y_blend_train)\n",
    "\n",
    "# Validation\n",
    "ada_blend_pred = ada_blend_model.predict(X_eval)\n",
    "print(ada_blend_pred[:10])\n",
    "print(Y_eval[:10])\n",
    "mae = np.mean(np.abs(ada_blend_pred - Y_eval))\n",
    "print(np.mean(np.abs(ada_blend_pred - Y_eval)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'model/ensemble_adaboost_blend_model_{mae}.pickle', 'wb') as f:\n",
    "    pickle.dump(ada_blend_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.670722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1093869\n",
      "[LightGBM] [Info] Number of data points in the train set: 13650, number of used features: 5393\n",
      "[LightGBM] [Info] Start training from score 4.548205\n",
      "[ 5.9479592   4.5484604   5.32569816  4.10126705  5.51138259  5.46663252\n",
      "  3.10486665 -0.01848302  3.52725796  5.6550889 ]\n",
      "[6 8 5 6 5 7 2 0 6 5]\n",
      "1.5016537392633487\n"
     ]
    }
   ],
   "source": [
    "# LigthGBM dart\n",
    "lgbm_dart_blend_model = LGBMRegressor(n_estimators=25000,\n",
    "                            verbose=1,\n",
    "                            n_jobs=10,\n",
    "                            feature_fraction=0.3,\n",
    "                            bagging_fraction=0.4,\n",
    "                            bagging_freq=10,\n",
    "                            random_state=1234,\n",
    "                            boosting_type='dart',\n",
    "                            metric='mae',)\n",
    "\n",
    "lgbm_dart_blend_model.fit(X_blend_train,Y_blend_train)\n",
    "\n",
    "# Validation dart\n",
    "dart_blend_pred = lgbm_dart_blend_model.predict(X_eval)\n",
    "print(dart_blend_pred[:10])\n",
    "print(Y_eval[:10])\n",
    "mae = np.mean(np.abs(dart_blend_pred - Y_eval))\n",
    "print(np.mean(np.abs(dart_blend_pred - Y_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'model/lgbm_dart_blend_model_{mae}.pickle', 'wb') as f:\n",
    "    pickle.dump(lgbm_dart_blend_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.851178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1093869\n",
      "[LightGBM] [Info] Number of data points in the train set: 13650, number of used features: 5393\n",
      "[LightGBM] [Info] Start training from score 4.548205\n",
      "[ 5.99474503  3.10608888  4.95041417  4.439354    5.47372553  5.79711771\n",
      "  2.54150961 -0.48164055  3.03441591  5.25761051]\n",
      "[6 8 5 6 5 7 2 0 6 5]\n",
      "1.5209615179935414\n"
     ]
    }
   ],
   "source": [
    "# LigthGBM gbdt\n",
    "lgbm_gbdt_blend_model = LGBMRegressor(n_estimators=2500,\n",
    "                            verbose=1,\n",
    "                            n_jobs=10,\n",
    "                            feature_fraction=0.3,\n",
    "                            bagging_fraction=0.4,\n",
    "                            random_state=1234,\n",
    "                            boosting_type='gbdt',\n",
    "                            metric='mae',\n",
    "                            )\n",
    "\n",
    "lgbm_gbdt_blend_model.fit(X_blend_train,Y_blend_train)\n",
    "\n",
    "# Validation\n",
    "gbdt_blend_pred = lgbm_gbdt_blend_model.predict(X_eval)\n",
    "print(gbdt_blend_pred[:10])\n",
    "print(Y_eval[:10])\n",
    "mae = np.mean(np.abs(gbdt_blend_pred - Y_eval))\n",
    "print(np.mean(np.abs(gbdt_blend_pred - Y_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.99474503  3.10608888  4.95041417  4.439354    5.47372553  5.79711771\n",
      "  2.54150961 -0.48164055  3.03441591  5.25761051]\n",
      "[6 8 5 6 5 7 2 0 6 5]\n",
      "1.5209615179935414\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "gbdt_blend_pred = lgbm_gbdt_blend_model.predict(X_eval)\n",
    "print(gbdt_blend_pred[:10])\n",
    "print(Y_eval[:10])\n",
    "mae = np.mean(np.abs(gbdt_blend_pred - Y_eval))\n",
    "print(np.mean(np.abs(gbdt_blend_pred - Y_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'model/lgbm_gbdt_blend_model_{mae}.pickle', 'wb') as f:\n",
    "    pickle.dump(lgbm_gbdt_blend_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_rf_blend_model = LGBMRegressor(n_estimators=25000,\n",
    "                            verbose=1,\n",
    "                            n_jobs=10,\n",
    "                            feature_fraction=0.3,\n",
    "                            bagging_fraction=0.4,\n",
    "                            bagging_freq=10,\n",
    "                            random_state=1234,\n",
    "                            boosting_type='rf',\n",
    "                            metric='mae',)\n",
    "lgbm_rf_blend_model.fit(X_train,Y_train)\n",
    "\n",
    "# Validation random forest\n",
    "rf_blend_pred = lgbm_rf_model.predict(X_eval)\n",
    "print(rf_blend_pred[:10])\n",
    "print(Y_eval[:10])\n",
    "mae = np.mean(np.abs(rf_blend_pred - Y_eval))\n",
    "print(np.mean(np.abs(rf_blend_pred - Y_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'model/lgbm_rf_model_{mae}.pickle', 'wb') as f:\n",
    "    pickle.dump(lgbm_rf_blend_model, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Search for best weights combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def postClean(label):\n",
    "    for i, label in enumerate(label):\n",
    "        if label[i] < 0:\n",
    "            label[i] = 0\n",
    "        elif label[i] > 9:\n",
    "            label[i] = 9\n",
    "        \n",
    "        label[i] = np.round(label[i])\n",
    "    return label\n",
    "@jit\n",
    "def LinearSearch(pred1, pred2, pred3, Y_eval, if_clean:dict):\n",
    "\n",
    "    # init search grid\n",
    "    model1_w = np.arange(1, 20, 1)\n",
    "    model2_w = np.arange(1, 20, 1)\n",
    "    model3_w = np.arange(1, 20, 1)\n",
    "    \n",
    "    \n",
    "    if if_clean['model1']:\n",
    "        for i, label in enumerate(pred1):\n",
    "            if pred1[i] < 0:\n",
    "                pred1[i] = 0\n",
    "            elif pred1[i] > 9:\n",
    "                pred1[i] = 9\n",
    "            \n",
    "            if if_clean['model1_round']:\n",
    "                pred1[i] = np.round(pred1[i])\n",
    "                \n",
    "    if if_clean['model2']:\n",
    "        for i, label in enumerate(pred2):\n",
    "            if pred2[i] < 0:\n",
    "                pred2[i] = 0\n",
    "            elif pred2[i] > 9:\n",
    "                pred2[i] = 9\n",
    "            \n",
    "            if if_clean['model2_round']:\n",
    "                pred2[i] = np.round(pred2[i])\n",
    "                \n",
    "    if if_clean['model3']:\n",
    "        for i, label in enumerate(pred3):\n",
    "            if pred3[i] < 0:\n",
    "                pred3[i] = 0\n",
    "            elif pred3[i] > 9:\n",
    "                pred3[i] = 9\n",
    "            \n",
    "            if if_clean['model3_round']:\n",
    "                pred3[i] = np.round(pred3[i])\n",
    "\n",
    "    if_post_clean = [True, False]\n",
    "\n",
    "    best_mae = 10\n",
    "    best_weights = {\n",
    "        'weight1': 0,\n",
    "        'weight2': 0,\n",
    "        'weight3': 0,\n",
    "        'post_clean': False,\n",
    "    }\n",
    "\n",
    "    for w1 in model1_w:\n",
    "        for w2 in model2_w:\n",
    "            for w3 in model3_w:\n",
    "                for post_clean in if_post_clean:\n",
    "                    ensemble_pred = (w1*pred1 + w2*pred2 + w3*pred3) / np.sum([w1, w2, w3])\n",
    "                    if post_clean:\n",
    "                        for i, label in enumerate(ensemble_pred):\n",
    "                            if ensemble_pred[i] < 0:\n",
    "                                ensemble_pred[i] = 0\n",
    "                            elif ensemble_pred[i] > 9:\n",
    "                                ensemble_pred[i] = 9\n",
    "                            \n",
    "                            ensemble_pred[i] = np.round(ensemble_pred[i])\n",
    "                            \n",
    "                    mae = np.mean(np.abs(ensemble_pred - Y_eval))\n",
    "                    if mae < best_mae:\n",
    "                        best_mae = mae\n",
    "                        best_weights['weight1'] = w1\n",
    "                        best_weights['weight2'] = w2\n",
    "                        best_weights['weight3'] = w3\n",
    "                        best_weights['post_clean'] = post_clean\n",
    "                        print(best_mae, best_weights)\n",
    "\n",
    "                    \n",
    "    return best_mae, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"LinearSearch\" failed type inference due to: non-precise type pyobject\n",
      "During: typing of argument at /var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py (5)\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 5:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  @jit\n",
      "/var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"LinearSearch\" failed type inference due to: cannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 12:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  @jit\n",
      "/Users/DAT/anaconda3/envs/myenv37/lib/python3.7/site-packages/numba/core/object_mode_passes.py:178: NumbaWarning: Function \"LinearSearch\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 5:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/Users/DAT/anaconda3/envs/myenv37/lib/python3.7/site-packages/numba/core/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 5:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"LinearSearch\" failed type inference due to: non-precise type pyobject\n",
      "During: typing of argument at /var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py (12)\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 12:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  @jit\n",
      "/Users/DAT/anaconda3/envs/myenv37/lib/python3.7/site-packages/numba/core/object_mode_passes.py:178: NumbaWarning: Function \"LinearSearch\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 12:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/Users/DAT/anaconda3/envs/myenv37/lib/python3.7/site-packages/numba/core/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 12:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"LinearSearch\" failed type inference due to: non-precise type pyobject\n",
      "During: typing of argument at /var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py (22)\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 22:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  @jit\n",
      "/Users/DAT/anaconda3/envs/myenv37/lib/python3.7/site-packages/numba/core/object_mode_passes.py:178: NumbaWarning: Function \"LinearSearch\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 22:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/Users/DAT/anaconda3/envs/myenv37/lib/python3.7/site-packages/numba/core/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 22:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"LinearSearch\" failed type inference due to: non-precise type pyobject\n",
      "During: typing of argument at /var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py (32)\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 32:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  @jit\n",
      "/Users/DAT/anaconda3/envs/myenv37/lib/python3.7/site-packages/numba/core/object_mode_passes.py:178: NumbaWarning: Function \"LinearSearch\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 32:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/Users/DAT/anaconda3/envs/myenv37/lib/python3.7/site-packages/numba/core/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 32:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"LinearSearch\" failed type inference due to: non-precise type pyobject\n",
      "During: typing of argument at /var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py (51)\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 51:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  @jit\n",
      "/Users/DAT/anaconda3/envs/myenv37/lib/python3.7/site-packages/numba/core/object_mode_passes.py:178: NumbaWarning: Function \"LinearSearch\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 51:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/Users/DAT/anaconda3/envs/myenv37/lib/python3.7/site-packages/numba/core/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"../../../../../../../../../../../var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/4275379307.py\", line 51:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4928533615669666 {'weight1': 1, 'weight2': 1, 'weight3': 1, 'post_clean': True}\n",
      "1.489853538027175 {'weight1': 1, 'weight2': 2, 'weight3': 1, 'post_clean': True}\n",
      "1.488912416916652 {'weight1': 1, 'weight2': 4, 'weight3': 1, 'post_clean': False}\n",
      "1.487030174695606 {'weight1': 1, 'weight2': 4, 'weight3': 2, 'post_clean': True}\n",
      "1.486696860968963 {'weight1': 1, 'weight2': 7, 'weight3': 1, 'post_clean': False}\n",
      "1.4861478736544909 {'weight1': 1, 'weight2': 7, 'weight3': 3, 'post_clean': True}\n",
      "1.4859714134462678 {'weight1': 1, 'weight2': 9, 'weight3': 1, 'post_clean': True}\n",
      "1.485955371609157 {'weight1': 1, 'weight2': 9, 'weight3': 1, 'post_clean': False}\n",
      "1.4842068113640374 {'weight1': 1, 'weight2': 10, 'weight3': 1, 'post_clean': True}\n",
      "1.4831480501146992 {'weight1': 1, 'weight2': 11, 'weight3': 1, 'post_clean': True}\n",
      "1.4829715899064762 {'weight1': 1, 'weight2': 12, 'weight3': 1, 'post_clean': True}\n",
      "1.482265749073584 {'weight1': 1, 'weight2': 13, 'weight3': 1, 'post_clean': True}\n",
      "1.482089288865361 {'weight1': 2, 'weight2': 3, 'weight3': 2, 'post_clean': True}\n",
      "1.4801482265749073 {'weight1': 3, 'weight2': 3, 'weight3': 1, 'post_clean': True}\n",
      "1.4780307040762308 {'weight1': 3, 'weight2': 4, 'weight3': 1, 'post_clean': True}\n",
      "1.4773248632433387 {'weight1': 3, 'weight2': 5, 'weight3': 1, 'post_clean': True}\n",
      "1.4739721192871007 {'weight1': 4, 'weight2': 4, 'weight3': 1, 'post_clean': True}\n",
      "1.4718545967884242 {'weight1': 5, 'weight2': 5, 'weight3': 1, 'post_clean': True}\n",
      "1.4697370742897478 {'weight1': 6, 'weight2': 6, 'weight3': 1, 'post_clean': True}\n",
      "1.4695606140815247 {'weight1': 7, 'weight2': 7, 'weight3': 1, 'post_clean': True}\n"
     ]
    }
   ],
   "source": [
    "# gbdt_pred = lgbm_gbdt_model.predict(X_eval)\n",
    "# dart_pred = lgbm_dart_model.predict(X_eval)\n",
    "# ada_pred = ada_model.predict(X_eval)\n",
    "if_clean = {\n",
    "    'model1': False,\n",
    "    'model1_round': False,\n",
    "    'model2': False,\n",
    "    'model2_round': False,\n",
    "    'model3': False,\n",
    "    'model3_round': False,\n",
    "}\n",
    "mae, best_weights = LinearSearch(gbdt_pred, dart_pred, ada_pred, Y_eval, if_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5667,) (4168,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/3789971677.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 更換為Y_test_eval調參\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmae_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbdt_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdart_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mada_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/n2/87d8k7hn75z50bb14qb5p1wh0000gn/T/ipykernel_2171/3474364004.py\u001b[0m in \u001b[0;36mLinearSearch\u001b[0;34m(pred1, pred2, pred3, Y_eval, if_clean)\u001b[0m\n\u001b[1;32m     73\u001b[0m                             \u001b[0mensemble_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensemble_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensemble_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmae\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_mae\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                         \u001b[0mbest_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5667,) (4168,) "
     ]
    }
   ],
   "source": [
    "# 更換為Y_test_eval調參\n",
    "mae_test, best_weights_test = LinearSearch(gbdt_pred, dart_pred, ada_pred, Y_test_eval, if_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4995588494794423 {'weight1': 1, 'weight2': 1, 'weight3': 1, 'post_clean': True}\n",
      "1.4914416799011823 {'weight1': 1, 'weight2': 2, 'weight3': 1, 'post_clean': True}\n",
      "1.481912828657138 {'weight1': 1, 'weight2': 3, 'weight3': 1, 'post_clean': True}\n",
      "1.478383624492677 {'weight1': 2, 'weight2': 3, 'weight3': 1, 'post_clean': True}\n",
      "1.474854420328216 {'weight1': 2, 'weight2': 4, 'weight3': 1, 'post_clean': True}\n",
      "1.4743250397035468 {'weight1': 3, 'weight2': 4, 'weight3': 1, 'post_clean': True}\n",
      "1.4699135344979708 {'weight1': 3, 'weight2': 5, 'weight3': 1, 'post_clean': True}\n",
      "1.4693841538733017 {'weight1': 4, 'weight2': 5, 'weight3': 1, 'post_clean': True}\n",
      "1.4685018528321863 {'weight1': 4, 'weight2': 6, 'weight3': 1, 'post_clean': True}\n",
      "1.4679724722075171 {'weight1': 5, 'weight2': 7, 'weight3': 1, 'post_clean': True}\n",
      "1.467443091582848 {'weight1': 6, 'weight2': 8, 'weight3': 1, 'post_clean': True}\n"
     ]
    }
   ],
   "source": [
    "# blend\n",
    "mae_blend, best_weights_blend = LinearSearch(gbdt_blend_pred, dart_blend_pred, ada_blend_pred, Y_eval, if_clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predcit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_pred_Test = lgbm_gbdt_model.predict(X_test)\n",
    "dart_pred_Test = lgbm_dart_model.predict(X_test)\n",
    "ada_pred_Test = ada_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight\n",
    "w1 = 1\n",
    "w2 = 9\n",
    "w3 = 1\n",
    "ensemble_pred_Test = (w1 * gbdt_pred_Test + w2 * dart_pred_Test + w3 * ada_pred_Test) / (w1+w2+w3)\n",
    "\n",
    "# print(best_weights)\n",
    "# ensemble_pred_Test = (best_weights['weight1'] * gbdt_pred_Test + best_weights['weight2'] * dart_pred_Test + best_weights['weight3'] * ada_pred_Test) / (best_weights['weight1']+best_weights['weight2']+best_weights['weight3'])\n",
    "\n",
    "# 清outliner和4捨5入\n",
    "# ensemble_pred_Test = postClean(ensemble_pred_Test)\n",
    "for i, label in enumerate(ensemble_pred_Test):\n",
    "    if ensemble_pred_Test[i] < 0:\n",
    "        ensemble_pred_Test[i] = 0\n",
    "    elif ensemble_pred_Test[i] > 9:\n",
    "        ensemble_pred_Test[i] = 9\n",
    "    \n",
    "    ensemble_pred_Test[i] = np.round(ensemble_pred_Test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4859714134462678"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_pred = (w1 * gbdt_pred + w2 * dart_pred + w3 * ada_pred) / (w1+w2+w3)\n",
    "for i, label in enumerate(ensemble_pred):\n",
    "    if ensemble_pred[i] < 0:\n",
    "        ensemble_pred[i] = 0\n",
    "    elif ensemble_pred[i] > 9:\n",
    "        ensemble_pred[i] = 9\n",
    "    \n",
    "    ensemble_pred[i] = np.round(ensemble_pred[i])\n",
    "mae = np.mean(np.abs(ensemble_pred - Y_eval))\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(columns=['id', 'Danceability'])\n",
    "test_df['id'] = testing_id\n",
    "test_df['Danceability'] = ensemble_pred_Test\n",
    "\n",
    "test_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_pred_Test_blend = lgbm_gbdt_blend_model.predict(X_test)\n",
    "dart_pred_Test_blend = lgbm_dart_blend_model.predict(X_test)\n",
    "ada_pred_Test_blend = ada_blend_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32636579572446556"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = 6\n",
    "w2 = 8\n",
    "w3 = 1\n",
    "ensemble_pred_Test_blend = (w1 * gbdt_pred_Test_blend + w2 * dart_pred_Test_blend + w3 * ada_pred_Test_blend) / (w1+w2+w3)\n",
    "\n",
    "for i, label in enumerate(ensemble_pred_Test_blend):\n",
    "    if ensemble_pred_Test_blend[i] < 0:\n",
    "        ensemble_pred_Test_blend[i] = 0\n",
    "    elif ensemble_pred_Test_blend[i] > 9:\n",
    "        ensemble_pred_Test_blend[i] = 9\n",
    "    \n",
    "    ensemble_pred_Test_blend[i] = np.round(ensemble_pred_Test_blend[i])\n",
    "\n",
    "mae = np.mean(np.abs(ensemble_pred_Test_blend - Y_test))\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(columns=['id', 'Danceability'])\n",
    "test_df['id'] = testing_id\n",
    "test_df['Danceability'] = ensemble_pred_Test_blend\n",
    "\n",
    "test_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
