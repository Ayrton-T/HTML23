{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations_with_replacement\n",
    "from liblinear.liblinearutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module liblinear.liblinearutil:\n",
      "\n",
      "train(arg1, arg2=None, arg3=None)\n",
      "    train(y, x [, options]) -> model | ACC\n",
      "    \n",
      "    y: a list/tuple/ndarray of l true labels (type must be int/double).\n",
      "    \n",
      "    x: 1. a list/tuple of l training instances. Feature vector of\n",
      "          each training instance is a list/tuple or dictionary.\n",
      "    \n",
      "       2. an l * n numpy ndarray or scipy spmatrix (n: number of features).\n",
      "    \n",
      "    train(prob [, options]) -> model | ACC\n",
      "    train(prob, param) -> model | ACC\n",
      "    \n",
      "    Train a model from data (y, x) or a problem prob using\n",
      "    'options' or a parameter param.\n",
      "    \n",
      "    If '-v' is specified in 'options' (i.e., cross validation)\n",
      "    either accuracy (ACC) or mean-squared error (MSE) is returned.\n",
      "    \n",
      "    options:\n",
      "        -s type : set type of solver (default 1)\n",
      "          for multi-class classification\n",
      "             0 -- L2-regularized logistic regression (primal)\n",
      "             1 -- L2-regularized L2-loss support vector classification (dual)\n",
      "             2 -- L2-regularized L2-loss support vector classification (primal)\n",
      "             3 -- L2-regularized L1-loss support vector classification (dual)\n",
      "             4 -- support vector classification by Crammer and Singer\n",
      "             5 -- L1-regularized L2-loss support vector classification\n",
      "             6 -- L1-regularized logistic regression\n",
      "             7 -- L2-regularized logistic regression (dual)\n",
      "          for regression\n",
      "            11 -- L2-regularized L2-loss support vector regression (primal)\n",
      "            12 -- L2-regularized L2-loss support vector regression (dual)\n",
      "            13 -- L2-regularized L1-loss support vector regression (dual)\n",
      "          for outlier detection\n",
      "            21 -- one-class support vector machine (dual)\n",
      "        -c cost : set the parameter C (default 1)\n",
      "        -p epsilon : set the epsilon in loss function of SVR (default 0.1)\n",
      "        -e epsilon : set tolerance of termination criterion\n",
      "            -s 0 and 2\n",
      "                |f'(w)|_2 <= eps*min(pos,neg)/l*|f'(w0)|_2,\n",
      "                where f is the primal function, (default 0.01)\n",
      "            -s 11\n",
      "                |f'(w)|_2 <= eps*|f'(w0)|_2 (default 0.0001)\n",
      "            -s 1, 3, 4, 7, and 21\n",
      "                Dual maximal violation <= eps; similar to libsvm (default 0.1 except 0.01 for -s 21)\n",
      "            -s 5 and 6\n",
      "                |f'(w)|_inf <= eps*min(pos,neg)/l*|f'(w0)|_inf,\n",
      "                where f is the primal function (default 0.01)\n",
      "            -s 12 and 13\n",
      "                |f'(alpha)|_1 <= eps |f'(alpha0)|,\n",
      "                where f is the dual function (default 0.1)\n",
      "        -B bias : if bias >= 0, instance x becomes [x; bias]; if < 0, no bias term added (default -1)\n",
      "        -R : not regularize the bias; must with -B 1 to have the bias; DON'T use this unless you know what it is\n",
      "            (for -s 0, 2, 5, 6, 11)\"\n",
      "        -wi weight: weights adjust the parameter C of different classes (see README for details)\n",
      "        -v n: n-fold cross validation mode\n",
      "        -C : find parameters (C for -s 0, 2 and C, p for -s 11)\n",
      "        -q : quiet mode (no outputs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_transformation(x, order):\n",
    "    n, index = x.shape[0], list(range(x.shape[1]))\n",
    "    new_x = np.ones((x.shape[0], 1))\n",
    "    for o in range(1, order+1):\n",
    "        idx_set = np.array(list(combinations_with_replacement(index, o)))\n",
    "        for set in idx_set:\n",
    "            new_col = np.ones((x.shape[0], 1))\n",
    "            for i in set:\n",
    "                new_col *= np.reshape(x[:,i], (n, 1))\n",
    "            new_x = np.hstack((new_x, new_col))\n",
    "    return new_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 10)\n",
      "(200, 1)\n",
      "(500, 10)\n",
      "(500, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "load data and preprocess\n",
    "\"\"\"\n",
    "# read data\n",
    "with open('hw4_train.dat', 'rb') as f:  \n",
    "    training_data = np.array([np.float64(i.split()) for i in f.readlines()])\n",
    "    \n",
    "# turn x and y into numpy array\n",
    "# x is the input feature vector space and y is the corresponding label\n",
    "x = np.array(training_data[:,0:10])\n",
    "y = np.reshape(np.array(list(map(int, training_data[:,10]))), (len(x), 1))\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "N = len(x)\n",
    "\n",
    "with open('hw4_test.dat', 'rb') as f:  \n",
    "    test_data = np.array([np.float64(i.split()) for i in f.readlines()])\n",
    "    \n",
    "# turn x and y into numpy array\n",
    "# x is the input feature vector space and y is the corresponding label\n",
    "x_test = np.array(test_data[:,0:10])\n",
    "y_test = np.reshape(np.array(list(map(int, test_data[:,10]))), (len(x_test), 1))\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "N = len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1001)\n",
      "(500, 1001)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "polynomial transformation\n",
    "\"\"\"\n",
    "order = 4\n",
    "x_transform = polynomial_transformation(x, order)\n",
    "x_test_transform = polynomial_transformation(x_test, order)\n",
    "\n",
    "# for i in range(x_transform.shape[0]):\n",
    "#     print(x_transform[i])\n",
    "    \n",
    "print(x_transform.shape)\n",
    "print(x_test_transform.shape)\n",
    "\n",
    "y = y.flatten()\n",
    "y_test = y_test.flatten()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 77.4% (387/500) (classification)\n",
      "Accuracy = 82.2% (411/500) (classification)\n",
      "Accuracy = 84.6% (423/500) (classification)\n",
      "Accuracy = 85.8% (429/500) (classification)\n",
      "Accuracy = 81.2% (406/500) (classification)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "P12\n",
    "\"\"\"\n",
    "\n",
    "lamb = np.array([-6, -3, 0, 3, 6], dtype=np.float32)\n",
    "prob = problem(y, x_transform)\n",
    "E_list = []\n",
    "for l in lamb:\n",
    "    parm = parameter(f'-s 0 -c {1/(2*(10**l))} -e 0.000001')\n",
    "    # print(1/(2*(10**l)))\n",
    "    model = train(prob, parm)\n",
    "    p_label, p_acc, p_val = predict(y_test, x_test_transform, model)\n",
    "    E_list.append(np.mean(p_label != y_test))\n",
    "\n",
    "best_e = 1.1\n",
    "best_lambda = -np.inf\n",
    "for idx, e in enumerate(E_list):\n",
    "    if e < best_e:\n",
    "        best_e = e\n",
    "        best_lambda = lamb[idx]\n",
    "    elif e == best_e and lamb[idx] > best_lambda:\n",
    "        best_lambda = lamb[idx]\n",
    "        best_idx = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6. -3.  0.  3.  6.]\n",
      "[0.226, 0.178, 0.154, 0.142, 0.188]\n",
      "best error = 0.142, best lambda = 3.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "p12 ans\n",
    "\"\"\"\n",
    "print(lamb)\n",
    "print(E_list)        \n",
    "print(f'best error = {best_e}, best lambda = {best_lambda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100% (200/200) (classification)\n",
      "Accuracy = 100% (200/200) (classification)\n",
      "Accuracy = 100% (200/200) (classification)\n",
      "Accuracy = 96% (192/200) (classification)\n",
      "Accuracy = 76% (152/200) (classification)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "P13\n",
    "\"\"\"\n",
    "lamb = np.array([-6, -3, 0, 3, 6], dtype=np.float32)\n",
    "prob = problem(y, x_transform)\n",
    "E_list = []\n",
    "for l in lamb:\n",
    "    parm = parameter(f'-s 0 -c {1/(2*(10**l))} -e 0.000001')\n",
    "    # print(1/(2*(10**l)))\n",
    "    model = train(prob, parm)\n",
    "    p_label, p_acc, p_val = predict(y, x_transform, model)\n",
    "    E_list.append(np.mean(p_label != y))\n",
    "\n",
    "best_e = 1.1\n",
    "best_lambda = -np.inf\n",
    "for idx, e in enumerate(E_list):\n",
    "    if e < best_e:\n",
    "        best_e = e\n",
    "        best_lambda = lamb[idx]\n",
    "    elif e == best_e and lamb[idx] > best_lambda:\n",
    "        best_lambda = lamb[idx]\n",
    "        best_idx = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6. -3.  0.  3.  6.]\n",
      "[0.0, 0.0, 0.0, 0.04, 0.24]\n",
      "best error = 0.0, best lambda = 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "p13 ans\n",
    "\"\"\"\n",
    "print(lamb)\n",
    "print(E_list)\n",
    "print(f'best error = {best_e}, best lambda = {best_lambda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "P14\n",
    "\"\"\"\n",
    "\n",
    "lamb = np.array([-6, -3, 0, 3, 6], dtype=np.float32)\n",
    "\n",
    "lamb_cnt = {}\n",
    "for l in lamb:\n",
    "    lamb_cnt[l] = 0\n",
    "\n",
    "for i in range(256):\n",
    "    # generate random numbers between 0 to len(x_transform) without repeat, this step is equal to shuffle the data\n",
    "    rng = np.random.default_rng()\n",
    "    numbers = rng.choice(len(x_transform), size=len(x_transform), replace=False)\n",
    "\n",
    "    # 120 tranining samples\n",
    "    x_train = x_transform[numbers[:120]]\n",
    "    y_train = y[numbers[:120]]\n",
    "    # 80 evaluation samples\n",
    "    x_eval = x_transform[numbers[120:]]\n",
    "    y_eval = y[numbers[120:]]\n",
    "    \n",
    "    E_list = []\n",
    "    \n",
    "    prob = problem(y_train, x_train)\n",
    "    for l in lamb:\n",
    "        parm = parameter(f'-s 0 -c {1/(2*(10**l))} -e 0.000001')\n",
    "        # print(1/(2*(10**l)))\n",
    "        model = train(prob, parm)\n",
    "        p_label, p_acc, p_val = predict(y_eval, x_eval, model)\n",
    "        E_list.append(np.mean(p_label != y_eval))\n",
    "\n",
    "    best_e = 1.1\n",
    "    best_lambda = -np.inf\n",
    "    for idx, e in enumerate(E_list):\n",
    "        if e < best_e:\n",
    "            best_e = e\n",
    "            best_lambda = lamb[idx]\n",
    "            best_idx = idx\n",
    "        elif e == best_e and lamb[idx] > best_lambda:\n",
    "            best_lambda = lamb[idx]\n",
    "            best_idx = idx\n",
    "            \n",
    "    # print(Eout_list)\n",
    "    # print(f'best error = {best_e}, best lambda = {best_lambda}')\n",
    "    lamb_cnt[best_lambda] += 1\n",
    "    \n",
    "    # I delete the training output(Accuracy part), cuz its too lengthy and cannot be removed by specify -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-6.0: 1, -3.0: 20, 0.0: 77, 3.0: 157, 6.0: 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "p14 ans\n",
    "\"\"\"\n",
    "print(lamb_cnt)\n",
    "max(lamb_cnt, key=lamb_cnt.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "P15\n",
    "\"\"\"\n",
    "\n",
    "lamb = np.array([-6, -3, 0, 3, 6], dtype=np.float32)\n",
    "\n",
    "lamb_cnt = {}\n",
    "for l in lamb:\n",
    "    lamb_cnt[l] = 0\n",
    "    \n",
    "p15_ans = []\n",
    "for i in range(256):\n",
    "    # generate random numbers between 0 to len(x_transform) without repeat, this step is equal to shuffle the data\n",
    "    rng = np.random.default_rng()\n",
    "    numbers = rng.choice(len(x_transform), size=len(x_transform), replace=False)\n",
    "\n",
    "    # 120 tranining samples\n",
    "    x_train = x_transform[numbers[:120]]\n",
    "    y_train = y[numbers[:120]]\n",
    "    # 80 evaluation samples\n",
    "    x_eval = x_transform[numbers[120:]]\n",
    "    y_eval = y[numbers[120:]]\n",
    "    \n",
    "    E_list = []\n",
    "    \n",
    "    prob = problem(y_train, x_train)\n",
    "    for l in lamb:\n",
    "        parm = parameter(f'-s 0 -c {1/(2*(10**l))} -e 0.000001')\n",
    "        # print(1/(2*(10**l)))\n",
    "        model = train(prob, parm)\n",
    "        p_label, p_acc, p_val = predict(y_eval, x_eval, model)\n",
    "        E_list.append(np.mean(p_label != y_eval))\n",
    "\n",
    "    best_e = 1.1\n",
    "    best_lambda = -np.inf\n",
    "    for idx, e in enumerate(E_list):\n",
    "        if e < best_e:\n",
    "            best_e = e\n",
    "            best_lambda = lamb[idx]\n",
    "            best_idx = idx\n",
    "        elif e == best_e and lamb[idx] > best_lambda:\n",
    "            best_lambda = lamb[idx]\n",
    "            best_idx = idx\n",
    "\n",
    "    prob = problem(y_train, x_train)\n",
    "    parm = parameter(f'-s 0 -c {1/(2*(10**best_lambda))} -e 0.000001')\n",
    "    model = train(prob, parm)\n",
    "    p_label, p_acc, p_val = predict(y_test, x_test_transform, model)\n",
    "    \n",
    "    p15_ans.append(np.mean(p_label != y_test))\n",
    "    # print(Eout_list)\n",
    "    # print(f'best error = {best_e}, best lambda = {best_lambda}')\n",
    "    lamb_cnt[best_lambda] += 1\n",
    "    \n",
    "    # I delete the training output(Accuracy part), cuz its too lengthy and cannot be removed by specify -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.167234375\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "p15 ans\n",
    "\"\"\"\n",
    "print(np.mean(p15_ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "P16\n",
    "\"\"\"\n",
    "\n",
    "lamb = np.array([-6, -3, 0, 3, 6], dtype=np.float32)\n",
    "\n",
    "lamb_cnt = {}\n",
    "for l in lamb:\n",
    "    lamb_cnt[l] = 0\n",
    "\n",
    "\n",
    "p16_ans = []\n",
    "for i in range(256):\n",
    "    # generate random numbers between 0 to len(x_transform) without repeat, this step is equal to shuffle the data\n",
    "    rng = np.random.default_rng()\n",
    "    numbers = rng.choice(len(x_transform), size=len(x_transform), replace=False)\n",
    "\n",
    "    # 120 tranining samples\n",
    "    x_train = x_transform[numbers[:120]]\n",
    "    y_train = y[numbers[:120]]\n",
    "    # 80 evaluation samples\n",
    "    x_eval = x_transform[numbers[120:]]\n",
    "    y_eval = y[numbers[120:]]\n",
    "    \n",
    "    E_list = []\n",
    "    \n",
    "    prob = problem(y_train, x_train)\n",
    "    for l in lamb:\n",
    "        parm = parameter(f'-s 0 -c {1/(2*(10**l))} -e 0.000001')\n",
    "        # print(1/(2*(10**l)))\n",
    "        model = train(prob, parm)\n",
    "        p_label, p_acc, p_val = predict(y_eval, x_eval, model)\n",
    "        E_list.append(np.mean(p_label != y_eval))\n",
    "\n",
    "    best_e = 1.1\n",
    "    best_lambda = -np.inf\n",
    "    for idx, e in enumerate(E_list):\n",
    "        if e < best_e:\n",
    "            best_e = e\n",
    "            best_lambda = lamb[idx]\n",
    "            best_idx = idx\n",
    "        elif e == best_e and lamb[idx] > best_lambda:\n",
    "            best_lambda = lamb[idx]\n",
    "            best_idx = idx\n",
    "    prob = problem(y, x_transform)\n",
    "    parm = parameter(f'-s 0 -c {1/(2*(10**best_lambda))} -e 0.000001')\n",
    "    model = train(prob, parm)\n",
    "    p_label, p_acc, p_val = predict(y_test, x_test_transform, model)\n",
    "    \n",
    "\n",
    "    p16_ans.append(np.mean(p_label != y_test))\n",
    "    lamb_cnt[best_lambda] += 1\n",
    "    # I delete the training output(Accuracy part), cuz its too lengthy and cannot be removed by specify -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14942968749999996\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "p16 ans\n",
    "\"\"\"\n",
    "print(np.mean(p16_ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "P17\n",
    "\"\"\"\n",
    "lamb = np.array([-6, -3, 0, 3, 6], dtype=np.float32)\n",
    "ans = []\n",
    "for i in range(256):\n",
    "    # generate random numbers between 0 to len(x_transform) without repeat, this step is equal to shuffle the data\n",
    "    rng = np.random.default_rng()\n",
    "    numbers = rng.choice(len(x_transform), size=len(x_transform), replace=False)\n",
    "    \n",
    "    fold_num = 5\n",
    "    # slice into n-folds\n",
    "    # length of each fold\n",
    "    length = int(len(numbers)/fold_num) \n",
    "    folds = []\n",
    "    for k in range(fold_num-1):\n",
    "        folds += [numbers[k*length:(k+1)*length]]\n",
    "    folds += [numbers[(fold_num-1)*length:len(numbers)]]\n",
    "    E = []\n",
    "\n",
    "    for l in lamb:\n",
    "        E_list = []\n",
    "        \n",
    "        for j in range(fold_num):\n",
    "            temp = folds.copy()\n",
    "            # print(len(folds))\n",
    "            valid_idx = np.array(temp.pop(j)).flatten()\n",
    "            training_idx = np.array(temp).flatten()\n",
    "            \n",
    "            training_sample = x_transform[training_idx]\n",
    "            training_y = y[training_idx]\n",
    "            valid_sample = x_transform[valid_idx]\n",
    "            valid_y = y[valid_idx]\n",
    "        \n",
    "            prob = problem(training_y, training_sample)\n",
    "            \n",
    "            parm = parameter(f'-s 0 -c {1/(2*(10**l))} -e 0.000001')\n",
    "            model = train(prob, parm)\n",
    "            p_label, p_acc, p_val = predict(valid_y, valid_sample, model)\n",
    "            E_list.append(np.mean(p_label != valid_y))\n",
    "        \n",
    "        E.append(np.mean(E_list))\n",
    "    ans.append(min(E))\n",
    "    # I delete the training output(Accuracy part), cuz its too lengthy and cannot be removed by specify -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13078125000000002\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "p17 ans\n",
    "\"\"\"\n",
    "print(np.mean(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 77.2% (386/500) (classification)\n",
      "Accuracy = 84.4% (422/500) (classification)\n",
      "Accuracy = 84.6% (423/500) (classification)\n",
      "Accuracy = 68% (340/500) (classification)\n",
      "Accuracy = 49.2% (246/500) (classification)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "P18\n",
    "\"\"\"\n",
    "lamb = np.array([-6, -3, 0, 3, 6], dtype=np.float32)\n",
    "prob = problem(y, x_transform)\n",
    "E_list = []\n",
    "for l in lamb:\n",
    "    parm = parameter(f'-s 6 -c {1/(10**l)} -e 0.000001')\n",
    "    model = train(prob, parm)\n",
    "    p_label, p_acc, p_val = predict(y_test, x_test_transform, model)\n",
    "    E_list.append(np.mean(p_label != y_test))\n",
    "\n",
    "best_e = 1.1\n",
    "best_lambda = -np.inf\n",
    "for idx, e in enumerate(E_list):\n",
    "    if e < best_e:\n",
    "        best_e = e\n",
    "        best_lambda = lamb[idx]\n",
    "    elif e == best_e and lamb[idx] > best_lambda:\n",
    "        best_lambda = lamb[idx]\n",
    "        best_idx = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6. -3.  0.  3.  6.]\n",
      "[0.228, 0.156, 0.154, 0.32, 0.508]\n",
      "best error = 0.154, best lambda = 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "p18 ans\n",
    "\"\"\"\n",
    "print(lamb)\n",
    "print(E_list)        \n",
    "print(f'best error = {best_e}, best lambda = {best_lambda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "P19\n",
    "\"\"\"\n",
    "# bast lambda from p18\n",
    "prob = problem(y, x_transform)\n",
    "parm = parameter(f'-s 6 -c {1/(10**best_lambda)} -e 0.000001')\n",
    "model = train(prob, parm)\n",
    "\n",
    "cnt = 0\n",
    "for i in range(x_transform.shape[1]):\n",
    "    if np.abs(model.w[i]) <= 10**-6:\n",
    "        cnt += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "p19 ans\n",
    "\"\"\"\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 77.4% (387/500) (classification)\n",
      "Accuracy = 82.2% (411/500) (classification)\n",
      "Accuracy = 84.6% (423/500) (classification)\n",
      "Accuracy = 85.8% (429/500) (classification)\n",
      "Accuracy = 81.2% (406/500) (classification)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "P20\n",
    "\"\"\"\n",
    "lamb = np.array([-6, -3, 0, 3, 6], dtype=np.float32)\n",
    "prob = problem(y, x_transform)\n",
    "E_list = []\n",
    "for l in lamb:\n",
    "    parm = parameter(f'-s 0 -c {1/(2*(10**l))} -e 0.000001')\n",
    "    # print(1/(2*(10**l)))\n",
    "    model = train(prob, parm)\n",
    "    p_label, p_acc, p_val = predict(y_test, x_test_transform, model)\n",
    "    E_list.append(np.mean(p_label != y_test))\n",
    "\n",
    "best_e = 1.1\n",
    "best_lambda = -np.inf\n",
    "for idx, e in enumerate(E_list):\n",
    "    if e < best_e:\n",
    "        best_e = e\n",
    "        best_lambda = lamb[idx]\n",
    "    elif e == best_e and lamb[idx] > best_lambda:\n",
    "        best_lambda = lamb[idx]\n",
    "        best_idx = idx\n",
    "\n",
    "prob = problem(y, x_transform)\n",
    "parm = parameter(f'-s 0 -c {1/(2*(10**best_lambda))} -e 0.000001')\n",
    "model = train(prob, parm)\n",
    "\n",
    "cnt = 0\n",
    "for i in range(x_transform.shape[1]):\n",
    "    if np.abs(model.w[i]) <= 10**-6:\n",
    "        cnt += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "p20 ans\n",
    "\"\"\"\n",
    "cnt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
